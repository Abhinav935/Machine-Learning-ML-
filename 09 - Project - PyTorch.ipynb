{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef5edb03",
   "metadata": {},
   "source": [
    "### Step 1: Install Torch\n",
    "- Execute the following cell which will install **torch** and **torchvision**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c19e8755",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8bc9a2",
   "metadata": {},
   "source": [
    "### Step 2: Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fca19d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3a4ae5",
   "metadata": {},
   "source": [
    "### Step 3: Download the CIFAR10 dataset\n",
    "- Excute the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73c6e5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to downloads/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f1d3af5da7f43f19082fa5bbc9d210c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting downloads/cifar-10-python.tar.gz to downloads/\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_path = 'downloads/'\n",
    "cifar10 = datasets.CIFAR10(data_path, train=True, download=True)\n",
    "cifar10_val = datasets.CIFAR10(data_path, train=False, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1bd0ca",
   "metadata": {},
   "source": [
    "### Step 4: Explore the dataset\n",
    "- See the type of **cifar10**\n",
    "- Get the length of **cifar10**\n",
    "- Assign image and label of **cifar10** at index 1000\n",
    "- Get the class name of label\n",
    "    - HINT: Use **cifar10.classes[label]** to get the name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a18c1717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.cifar.CIFAR10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cifar10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5494a539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cifar10), len(cifar10_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f74c3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = cifar10[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cef4468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'truck'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar10.classes[label]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65639015",
   "metadata": {},
   "source": [
    "### Step 5: Visualize the image\n",
    "- Use **matplotlib** to visuazlize image\n",
    "    - HINT: just use **plt.imshow(...)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dbf5fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc6cbd0f550>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1e704f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8005d75d",
   "metadata": {},
   "source": [
    "### Step 6: Transform images\n",
    "- We need to convert the PIL image to a PyTorch tensor\n",
    "- We can easily transform it by adding **transform=transforms.ToTensor()** when reading the dataset.\n",
    "- This is given below (just execute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d7c8a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_cifar10 = datasets.CIFAR10(data_path, train=True, download=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dac2ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe65a981",
   "metadata": {},
   "source": [
    "### Step 7: Normalize images\n",
    "- Now you have all images (transformed) in **tensor_cifar10**.\n",
    "- To concatenate a stack of images use **torch.stack(..., dim=3)** on the images\n",
    "    - HINT: Use list comprehension to get a list of images from **tensor_cifar10** (to exclude labels)\n",
    "- Calculate the **mean(dim=1)** by applying it on the stack\n",
    "- Calculate the **std(dim=1)** by applying it on the stack\n",
    "- We will use the results in next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f02d859",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = torch.stack([img_t for img_t, _ in tensor_cifar10], dim=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9707c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4914, 0.4822, 0.4465])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs.view(3, -1).mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5789d4b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2470, 0.2435, 0.2616])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs.view(3, -1).std(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d298703f",
   "metadata": {},
   "source": [
    "### Step 8: Normalize the data\n",
    "- We can add a normalize transform with adding a **transforms.Compose([...])**, where the list will contain the transforms.\n",
    "- The transform we want are **transforms.ToTensor()** and **transforms.Normalize(...)**\n",
    "    - HINT: See lesson how it was done\n",
    "- The **transforms.Normalize(...)** takes two tuples of the results from last step.\n",
    "    - Note: that in the lesson it was single numbers, here we hare tuples.\n",
    "- Read the datasets to **cifar10** with the new transform\n",
    "- Read the validation dataset to **cifar10_val** with the new transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "069a5ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10 = datasets.CIFAR10(data_path, train=True, download=False, \n",
    "                           transform=transforms.Compose([\n",
    "                                                         transforms.ToTensor(),\n",
    "                                                         transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                                                               (0.2470, 0.2435, 0.2616))\n",
    "                           ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "037d04db",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_val = datasets.CIFAR10(data_path, train=False, download=False, \n",
    "                           transform=transforms.Compose([\n",
    "                                                         transforms.ToTensor(),\n",
    "                                                         transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                                                               (0.2470, 0.2435, 0.2616))\n",
    "                           ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ef2dcd",
   "metadata": {},
   "source": [
    "### Step 9: Limit the dataset\n",
    "- There are 10 classes in this dataset - to simplify, we will reduce it to two\n",
    "- We will keep label 0 and 2 (**'airplane'** and **'bird'**)\n",
    "- Use list comprehension to filter the datasets.\n",
    "    - To simplify use a **label_map = {0: 0, 2: 1}**, which is used to map label 0 to 0 and label 2 to 1.\n",
    "    - Then use list comprehension **[(img, label_map[label]) for img, label in cifar10 if label in [0, 2]]**\n",
    "    - And similar for **cifar10_val**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d81727b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: 0, 2: 1}\n",
    "cifar2 = [(img, label_map[label]) for img, label in cifar10 if label in [0, 2]]\n",
    "cifar2_val = [(img, label_map[label]) for img, label in cifar10_val if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8099214e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60bb3eac",
   "metadata": {},
   "source": [
    "### Step 10: Create the model\n",
    "- We create a simple model here\n",
    "    - 3072 input nodes -> Linear with 512 nodes (Tanh acitivation)  -> Linear with 2 nodes (LogSoftmax activation)\n",
    "- To do that use **nn.Sequential(...)** with the following arguments.\n",
    "    - **nn.Linear(3072, 512)**\n",
    "        - Bonus question: Why 3072 input nodes?\n",
    "    - **nn.Tanh()**\n",
    "    - **nn.Linear(512, 2)**\n",
    "    - **nn.LogSoftmax(dim=1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a34ddf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(3072, 512),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(512, 2),\n",
    "                      nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e4421c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf32440f",
   "metadata": {},
   "source": [
    "### Step 11: Train the model\n",
    "- Prepare training data\n",
    "\n",
    "```Python\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                           shuffle=True)\n",
    "\n",
    "```\n",
    "\n",
    "- Set the **learning_rate = 0.01** (to make it easy to adjust)\n",
    "- Prepare optimizer and loss function.\n",
    "\n",
    "```Python\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.NLLLoss()\n",
    "```\n",
    "\n",
    "- Run the training\n",
    "\n",
    "```Python\n",
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model(imgs.view(batch_size, -1))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0571922e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.832160\n",
      "Epoch: 1, Loss: 0.517345\n",
      "Epoch: 2, Loss: 0.416617\n",
      "Epoch: 3, Loss: 0.324232\n",
      "Epoch: 4, Loss: 0.309863\n",
      "Epoch: 5, Loss: 0.291439\n",
      "Epoch: 6, Loss: 0.339071\n",
      "Epoch: 7, Loss: 0.313071\n",
      "Epoch: 8, Loss: 0.233211\n",
      "Epoch: 9, Loss: 0.463295\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                           shuffle=True)\n",
    "learning_rate = 0.01\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model(imgs.view(batch_size, -1))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf47f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92a127ed",
   "metadata": {},
   "source": [
    "### Step 12: Test the model\n",
    "- Run the following code (where we assume the test data is called **cifar10_val** and the model **model**.\n",
    "```Python\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\n",
    "                                         shuffle=False)\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model(imgs.view(batch_size, -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "print(\"Accuracy: %f\", correct / total)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0239d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: %f 0.819\n"
     ]
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\n",
    "                                       shuffle=False)\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "  for imgs, labels in val_loader:\n",
    "      batch_size = imgs.shape[0]\n",
    "      outputs = model(imgs.view(batch_size, -1))\n",
    "      _, predicted = torch.max(outputs, dim=1)\n",
    "      total += labels.shape[0]\n",
    "      correct += int((predicted == labels).sum())\n",
    "print(\"Accuracy: %f\", correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f167b492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99161df7",
   "metadata": {},
   "source": [
    "### Step 13 (Optional): Improve the model\n",
    "- Try to improve the model\n",
    "    - Simple things you can play with\n",
    "        - Adjust the learning rate\n",
    "        - Run more epochs\n",
    "        - Number of hidden nodes\n",
    "    - Medium things to play with\n",
    "        - Change activation functions\n",
    "        - Add another layer\n",
    "    - Advanced things\n",
    "        - Let your imagination guide you\n",
    "        - For inspiration see state of the art results ([wiki](https://en.wikipedia.org/wiki/CIFAR-10#Research_papers_claiming_state-of-the-art_results_on_CIFAR-10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37270489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281083ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f103c9c",
   "metadata": {},
   "source": [
    "### Step 14 (Optional): Add more classes\n",
    "- The dataset was limited to two classes (**airplane**s and **bird**s)\n",
    "- Try to add another class (or more) and see how it changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cad0bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d5ffae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
